server:
  host: 0.0.0.0
  http_port: 8080
  max_concurrent: 1024
  enable_metrics: true
  enable_tracing: false

model:
  repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
  path: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
  format: gguf
  quantization: q4_k_m
  tokenizer: sentencepiece

models:
  - id: tinyllama
    path: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
    backend: cuda
    default: true

runtime:
  backend_priority: [cuda, cpu]
  backend_exposure:
    prefer_native: true
    allow_universal_fallback: true
  tensor_parallel: 1
  mps_layers: 0
  fairness:
    enable_preemption: true
    high_priority_threshold: 5
    max_timeslice_tokens: 64
  speculative_decoding:
    enabled: false
    draft_model: ""
    max_prefill_tokens: 256
    chunk_size: 4
  nvme_offload:
    path: ""
    workers: 1
    queue_depth: 256
  paged_kv:
    cpu_pages: 4096
    eviction: lru

adapters: []

auth:
  api_keys:
    - key: dev-key-123
      scopes: [generate, read, admin]
    - key: user-key-123
      scopes: [generate, read]
  rate_limit_per_minute: 120
  oidc_issuer: https://issuer.example.com
  oidc_audience: inferflux

guardrails:
  blocklist:
    - secret
    - confidential
  opa_endpoint: ""

logging:
  level: info
  format: json
  audit_log: logs/audit.log

registry:
  path: ""
  poll_interval_ms: 5000
